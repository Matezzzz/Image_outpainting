\chapter{Outpainting} \label{outpainting}


We now describe how to perform outpainting using the~trained models. We assume we start with an~image of the~same size as the~tokenizer input and want to expand it beyond its borders. We allow specifying the~\texttt{outpaint\_range} and \texttt{samples} parameters - the~outpaint range defines how many outpainting steps the~model should take, and the~samples define the~quality of decoding. We use the~tokenizer to convert the~input image to tokens, then we use the~MaskGIT to perform outpainting on the~created tokens, and finally, we use the~super sampler to generate high-quality samples. We visualize the~whole process in \figref{algorithm}, each step is described in more detail below.

\figureimg{algorithm}{An overview of the~outpainting algorithm. MaskGIT, tokenizer and super sampler are called multiple times during the~last three steps, to cover the~whole outpainted image.}


After converting the~input image we reserve space around it for the~tokens we want to generate and set them all to \texttt{MASK\_TOKEN}. After that, we move around the~perimeter of known tokens in the~order from \figref{outpainting_order} and use MaskGIT to generate new tokens around it, until the~whole space is filled. We found out that our MaskGIT model was quite unstable when used on a different token distribution than the default images - there are some tokens that naturally fill the whole image, for example dark blue or black, which mostly occur in monochrome dark images. When the model is unsure of which token should be placed, it can generate one of these at random, which will then cause the whole image to be filled with dark blue or black, no matter the surroundings. For this reason, we experimentally pinpoint several of these tokens that tend to fill a whole image when generated, and forbid the model from generating them. We refer to this practice as token blocking later on. We believe that the reason the model does not generalize well is that as all images from webcams are oriented approximately horizontally, the model has a relatively small range of freedom in movement when outpainting upwards and downwards, making generating tokens beyond this limit much harder.

\figureimg{outpainting_order}{Outpainting order. We start with known tokens in the~middle (white) and unknown everywhere else (black). Gray tokens are the~ones currently being outpainted, light gray are known tokens currently being fed into the~MaskGIT to create new ones.}


When all tokens have been generated, we want to convert them back to an~image. Because calling the~tokenizer decoder directly on neighboring blocks of tokens can introduce artifacts (as we have no guarantee that the~borders will be smooth), we instead use a~sliding window, call the~decoder with many overlapping squares of tokens, and average the~results. The~centers of the~generating blocks are on a~regular grid, and the~*samples* parameter specifies how many more times we call the~decoder, relative to neighboring blocks. This process produces multiple overlapping images - to compute the~final image, we use a~weighted averaging of the~colors on a~given position. Using weights equal to 1 everywhere can sometimes still produce visible edges in the~final image in places where one block ends. To eliminate this, we use smaller weights for colors near the~edge of their output image, while those at the~center have large ones.

The previous step created an~outpainted image, but due to averaging and data loss when using the~tokenizer, the~output is often blurry. We fix this with the~last network, the~super sampler, which can add details to the~outpainted image and upscale it. We once again work on a~grid, this time, we only introduce small overlaps to avoid inconsistencies between neighboring parts of the~image. Then we gradually go over all grid positions and call the~super sampler for each one. If we already know some part of the~result, because it was computed before, we alter the~diffusion model generation slightly - during each decoding step, when we separate the~edges into noise and signal data, and estimate the~image in the~next step using the~two, we replace the~estimated signal values with the~values we already know. After every position is done, we are left with the~upscaled image.
