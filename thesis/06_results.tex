\chapter{Results} \label{results}


We present the~hyperparameters we used to train all three models, and the~results achieved during training and when running the~whole algorithm.

We show the~parameters used to train all three models in \figref{model_parameters}, then present the~results for each model separately.


\figureimg{model_parameters}{Model parameters. We stop training manually after the~validation loss stops decreasing - causing fractional amount of epochs sometimes. The~super sampler has been trained on a~smaller number of images, but we didn't expect any significant improvement on the~full one, so we didn't retrain it on the~full dataset.}


\section{Tokenizer results}

The tokenizer gets an~image as input, converts it to tokens and back, losing some information in the~process, we are mainly interested in how much information was lost during the~conversion. First, we present sample images and their reconstructions in \figref{tokenizer_results}, then, the~loss progression during training below in \figref{tokenizer_losses}.

\figureimg{tokenizer_results}{Tokenizer results. The~top row consists of dataset images, the~second one contains reconstructed ones using the tokenizer. Third and fourth row show the~contents of the~red boundaries in the~original and reconstruction respectively, showing the~loss of local details, images are significantly more blurry.}

\figureimg{tokenizer_losses}{Tokenizer losses during training. The~solid line shows smoothed values using the~exponential moving average technique with $k=0.97$. The~left graph shows total validation loss (sum of VQVAE reconstruction loss, embedding loss, commitment loss and entropy loss), the~right shows reconstruction loss.}

\FloatBarrier

\section{MaskGIT results}

We train the~model to predict the~masked tokens. The~results we illustrate represent an~image being converted to tokens using a~tokenizer encoder, hiding some tokens by applying any MaskGIT training mask, using the~MaskGIT to reconstruct the~missing tokens, then using the~tokenizer decoder to convert the~result back to an~image. We show both the~results when decoding during one step, and when using \texttt{decode\_steps}$=12$ and \texttt{generation\_temperature}$=1.0$, in \figref{maskgit_results_1_2} for border masks and in \figref{maskgit_results_1_4} for corner masks.

\figureimg{maskgit_results_1_2}{MaskGIT results, border masks. The~first column shows images, just passed through the~tokenizer. The~second column shows which tokens (black) we discard before attempting to reconstruct them using MaskGIT. The~third column shows how MaskGIT can reconstruct the~missing parts in one step, and the~last column shows how it can recreate them using 12 steps, with the~generation temperature set to 1.0. The~last column contains relatively few details, this is due to the~fairly low generation temperature. We perform experiments with various values when analyzing the~outpainting results.}

\figureimg{maskgit_results_1_4}{MaskGIT results, corner masks. Columns have the~same meaning as in \figref{maskgit_results_1_2}}

During training, we track multiple metrics, most importantly validation accuracy on predicted tokens and validation loss, presented in \figref{maskgit_losses}.

\figureimg{maskgit_losses}{MaskGIT losses. We track the~validation accuracy when estimating all tokens discarded according to a~training mask and validaton loss on the~same set of tokens.}


\FloatBarrier

\section{Super sampler results}

For the~super sampler results, we present the~original image, the~low-resolution version, and how we manage to recreate the~upscaled image, in \figref{upscaler_results}.

\figureimg{upscaler_results}{Super sampler results. The~first shows the~original image, downscaled, then passed through the tokenizer and upscaled using bicubic interpolation. The second row is the same, but we use our model for the upscaling. The third row shows the original image without any changes. The next three rows contain contents of the red bounds in the original images, the order is the same.}

During training, the~most important metric is the~validation loss when predicting noise in an~image - during training, it develops as shown in \figref{upscaler_loss}.

\figureimg{upscaler_loss}{Validation noise loss during super sampler training.}


\FloatBarrier

\section{Outpainting results}

In this section, we will present the ideal parameters for our outpainting algorithm, detailed results, and finally, how it compares to other machine learning solutions capable of outpainting.

\subsection{Selecting parameters}

In this section, we seek to select the outpainting parameters in a way that maximizes the performance of our outpainting algorithm. Our experiments have shown that results do not differ for all values of the \texttt{samples} parameter greater than one, so we use \texttt{samples}$=2$ in all our experiments, as it requires the least processing power and makes the result less blurry. We try three values for the \texttt{generation\_temperature} parameter - $1$, $4$, and $10$, and show the results in \figref{generation_temperature_testing}.

\figureimg{generation_temperature_testing}{Testing different values of the \texttt{generation\_temperature} parameter for outpainting. The first row shows the images which we perform outpainting on, the others several different generation temperatures and their results, where the image we outpaint is always placed at the center. Each image was selected out of four outpainted images for each input.}

We believe generation temperature $t=4$ to be the best default setting, as it is able to create much more details than $t=2$, while still avoiding the noise and strange clouds sometimes present when using $t=10$. For this reason, we use $t=4$ in all subsequent figures.


\subsection{Our results}

We demonstrate the outpainting performance including all outpainted images for a given input in \figref{outpainting_results_1} and \figref{outpainting_results_2}. We use the best parameters found in the previous section, \texttt{samples}$=2$, \texttt{generation\_temp}$=4$. 

\figureimg{outpainting_results_1}{Outpainting results, part 1. The first row shows the image we outpaint, the other four rows show possible outpaintings. There are some inconsistencies compared to the real sky - the outpainted images in last three columns contain a large monochrome area at the bottom, which we would argue is not very probable in nature, but outside of that, they look quite realistic.}

\figureimg{outpainting_results_2}{Outpainting results, part 2. All images hold the same meaning as \figref{outpainting_results_1}. There are a few more inconsistencies, for example weird bottom parts in the first column or oversaturated parts in the third. But overall, clouds look quite good.}



\subsection{Comparing to other models}

In this section, we compare our algorithm against two popular image generation models that support sky outpainting, namely DALL-E 2 \citep{dalle_2_web} and Stable diffusion \citep{stable_diffusion_web} in \figref{outpainting_comparison_1} and \figref{outpainting_comparison_2}. For our model, we choose the best result of four tries, for DALLE-2 and stable diffusion, we choose the best of four for each side. We always use the prompt 'Photo of a sky' when performing outpainting. We use a~third party stable diffusion outpainting implementation freely available at \citep{stable_diffusion_editor}, and the official DALL-E 2.




\figureimg{outpainting_comparison_1}{Outpainting comparison, part 1. We show the input, then how our outpainting compares to Stable diffusion \citep{stable_diffusion_editor} and DALL-E 2 \citep{dalle_2_web} on the same input, with prompt 'Photo of a sky'. Stable diffusion is prone to adding unrequested objects to the side of the image in some of the four tries, we always select the option where the object blocks the smallest part of the sky.}

\figureimg{outpainting_comparison_2}{Outpainting comparison, part 2. Comparing another two images. Borders hold the same meaning as in \figref{outpainting_comparison_1}}

We believe that our technique is better than stable diffusion, while being behind DALL-E 2 in most areas. We present a short summary of our observations regarding the pros and cons of all models below:

\begin{itemize}
    \item Our outpainting algorithm. It never introduces additional objects into the scene, images sometimes lack details, because generation itself runs in a lower resolution, and we only upscale the image. It has a tendency to create largely monochrome images sometimes, this could be prevented by using a larger generation temperature, at the cost of introducing strange clouds.
    \item Stable diffusion. Generated images are a bit more diverse, and the generator doesn't converge to monochrome images. However, it is much less consistent - sometimes, unrequested objects are added to the outpainted image, or the result has a visibly different hue.
    \item DALLE-2. During our testing, the images created by DALL-E 2 surpassed the quality of those created by our method or by stable diffusion. Most of the time, all 4 options show feasible cloud configurations, local details are really good as well, random objects are present only rarely. The only area DALL-E 2 loses in is that it is not open-source, unlike the previous two algorithms.
\end{itemize}

