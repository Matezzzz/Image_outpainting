\chapter{Dataset} \label{dataset}


In this chapter, we describe how we prepare training data for all of the~models we will eventually use for outpainting.


\section{Dataset}

For training the~models, we use the~data graciously provided by the~Czech hydro-meteorological Institute (further referred to as CHMI) \citep{chmi_cz_web} and collected by the~computer graphics group \citep{computer_graphics_group_web} at Charles University. The~data consists of 98 locations, each of which has a~static webcam that saves an~image of resolution $1600 \times 1200$ every 5 minutes. We use 94 of 98 locations available, excluding some with images of wrong dimensions or low-quality data. For each of them, images were being collected from March in the~year 2021 to the~time of writing, summing up to circa 200 thousand images per location, or approximately 19 million in total. We show some images from the~dataset in \figref{dataset_examples}.

\figureimg{dataset_examples}{Unprocessed dataset examples. All contain the~landscape and the~sky, and an~overlay with the~CHMI logo and weather text.}


All images contain some part of the~landscape, the~CHMI logo in the~left top corner, and some weather measurements in the~top right, in addition to the~sky. For this reason, we start by performing a~simple segmentation on each location, figuring out which pixels are part of the~sky and which ones are not, assuming that these do not change over the~time of collecting the~data. These masks are generated once before training, and their creation is described in more detail in the~section below. When preparing the~dataset for training a~particular model, we downscale the~images and masks and select a~random part of the~sky as model input, then filter the~selected parts to condition the~model to create more diverse images. In the~following subsections, we first describe the~segmentation algorithm in detail, then we describe how it is used to generate training data for each model.


\section{Location segmentation}

We base location segmentation on the~following observation - during the~day, the~sky will change quite a~lot, while the~landscape stays mostly the~same. The~algorithm then boils down to detecting edges in an~image, doing some processing, and then finding the~places where edges are present many times during different times of the~day. We describe the~process of creating a~segmentation mask, in which unmasked pixels are part of the~sky, and masked ones are part of the~landscape, the~CHMI logo, or the~weather information text.

We detect edges by computing the~image gradient in the~direction of the~x and y axis and summing the~absolute values of both directions and all RGB components. After that, we use a~threshold, marking all values above it as parts of the~landscape.

We then process the~edge mask as follows - first, we perform the~\textit{morphological closing operation}\footnote{A combination of the~binary dilation and erosion operations. Erosion places the~mask at all points in the~image, and when there is a~below the~kernel that isn't true, it sets the~result for the~point as false. Dilation does the~opposite, setting a~pixel as true if any value below the~kernel is true.} with a~$3 \times 3$ square kernel five times in a~row, making parts of the~mask a~lot more cohesive and filling in noisy areas. After that, we find all continuous unmasked areas in the~mask and mask all areas whose number of pixels is below a~certain threshold ($20000$ pixels is used). Then, we do the~same for masked areas, discarding those with less than $300$ pixels, considering them random noise.

After this, we hide the~CHMI logo by masking a~$270 \times 115$ rectangle in the~top right and the~weather measurements by masking a~$250 \times 250$ rectangle in the~top left. The~process is shown in \figref{segmentation_single_images}.

\figureimg{segmentation_single_images}{Image segmentation on one image. Pixels colored red are parts of the~mask. Gradient mask is obtained by masking all areas with a~large enough gradient in the~original image, closed mask by doing the~closing operation, filled mask by filling small unmasked areas, discarded by discarding masked ones, masked sides by hiding the~CHMI logo and the~weather information.}

During daytime with bright skies, this generally produces an~acceptable mask, with just a~few mistakes, such as masked parts of clouds and missing masks over patches of the~landscape of the~same color. To fix these issues, we compute separate masks for a~hundred consecutive time steps from one day (starting at noon so most time steps are bright enough to distinguish details). Then, for each pixel, we compute the~largest consecutive amount of time steps it has been marked as an~edge and the~percentage of the~total time it was an~edge. For an~edge to be in the~final mask, the~consecutive amount must be above a~certain threshold, and the~percentage must be above another threshold.

We didn't manage to find thresholds that would work well for all locations and weather conditions, so we instead chose the~thresholds manually for all locations. The~final segmentation masks are presented in \figref{segmentation_final}.

\figureimg{segmentation_final}{Final segmentation. We show one sample image, the~resulting mask, and the~image with mask overlay for one location, Brno.}


\section{Generating training data}

We now describe how to generate a~training example, assuming we have a~loaded image and its' segmentation mask, both of size $1600 \times 1200$. When mentioning color constants in the~following paragraphs, we assume all images to be normalized with values between 0 and 1.

We generate training data by downscaling both the~image and the~mask by some factor based on the~model being trained, and selecting a~random subset that contains only sky pixels according to the~provided mask. We believe this is better than simply scaling the~image or using any part of it as it enables us to eliminate unwanted stretching artifacts while not having to worry about dealing with landscape pixels during training. It also conditions the~networks to learn exclusively about the~sky, which is what we want to generate during outpainting. The~tokenizer and MaskGIT networks take inputs of size $128 \times 128$ so we found it useful to downscale the~images by a~factor of $4$ before selecting the~area, allowing the~training to select a~large part of the~sky while still fitting multiple positions above the~landscape. As the~super sampler takes images of size $512 \times 512$, we don't downscale at all.

We also found it useful to filter the~images created by the~procedure above to condition networks to generate more interesting images. First, we discard all images that were recorded during the~night - because the~downscaled training images don't have a~high enough resolution to capture any stars or planets in the~night sky, all the~images are pitch black, and there is nothing of interest to generate. We consider all images with a~mean less than $0.2$ to be those of the~night sky, and discard all of them, filtering out about $40\%$ of all input images.

We also try to increase variability in the~generated images by limiting the~number of monochrome images present in the~training data. We found that using the~unrestricted dataset makes it highly likely that we will get a~single-color image during outpainting as well. Because we want to drastically limit this behavior, but not eliminate generating monochrome images altogether, we choose to keep $10\%$ of the~monochrome images with a~standard deviation from their mean of less than $0.05$, and all others, filtering out another $50\%$ of the~dataset. 

Although we eliminate quite a~large chunk of the~training data during this step, we find it worth the~increased quality in generated samples, and no measurable decrease in performance, partly due to the~abundance of data available. The~example data from each part of the~process is shown in \figref{dataset_filtering}.

\figureimg{dataset_filtering}{Dataset filtering. The~first row shows images from the~unfiltered dataset, the~second one from the~dataset with night images filtered out, and the~third row shows the~final dataset, with both night and monochrome images discarded.}


